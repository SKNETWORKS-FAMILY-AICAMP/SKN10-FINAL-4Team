import yt_dlp
import os
import sys
import numpy as np
import soundfile as sf
import pyloudnorm as pyln
import requests
import json
import subprocess
import webrtcvad
import struct
import wave
import librosa
from scipy.spatial.distance import cdist

def extract_transcript(youtube_url):
    """
    ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ
    """
    print("ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ ì¤‘...")
    
    ydl_opts = {
        'writesubtitles': True,
        'writeautomaticsub': True,
        'subtitleslangs': ['ko', 'en'],  # ì–¸ì–´
        'skip_download': True, 
        'quiet': True
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            # ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            info = ydl.extract_info(youtube_url, download=False)
            
            subtitles = info.get('subtitles', {})
            auto_subs = info.get('automatic_captions', {})
            
            # Combine manual and automatic subtitles
            all_subs = {}
            for lang in ['ko', 'en']:
                if lang in subtitles:
                    all_subs[lang] = subtitles[lang]
                elif lang in auto_subs:
                    all_subs[lang] = auto_subs[lang]
            
            if not all_subs:
                print("ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            best_lang = None
            if 'ko' in all_subs:
                best_lang = 'ko'
            elif 'en' in all_subs:
                best_lang = 'en'
            else:
                best_lang = list(all_subs.keys())[0]
            
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ìë§‰ ì–¸ì–´: {list(all_subs.keys())}")
            print(f"ì„ íƒëœ ì–¸ì–´: {best_lang}")
            
            # ìë§‰ ë‹¤ìš´ë¡œë“œ
            sub_url = all_subs[best_lang][0]['url']
            subtitle_response = requests.get(sub_url)
            subtitle_response.raise_for_status()
            
            # ìë§‰ íŒŒì‹± (JSON í˜•ì‹ìœ¼ë¡œ ê°€ì •)
            subtitle_data = subtitle_response.json()
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆëŠ” ì´ë²¤íŠ¸ ì¶”ì¶œ
            events = subtitle_data.get('events', [])
            transcript = []
            
            for event in events:
                if 'segs' in event:
                    start_time = event.get('tStartMs', 0) / 1000.0  # Convert to seconds
                    end_time = start_time + (event.get('dDurationMs', 0) / 1000.0)
                    
                    # Combine all text segments
                    text = ''
                    for seg in event['segs']:
                        text += seg.get('utf8', '')
                    
                    if text.strip():  # Only add non-empty segments
                        transcript.append({
                            'start': start_time,
                            'end': end_time,
                            'text': text.strip()
                        })
            
            print(f"ìë§‰ ì¶”ì¶œ ì™„ë£Œ: {len(transcript)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            
            # Print first few segments as preview
            if transcript:
                print("\nìë§‰ ë¯¸ë¦¬ë³´ê¸°:")
                for i, segment in enumerate(transcript[:3]):
                    print(f"{segment['start']:.1f}s - {segment['end']:.1f}s: {segment['text']}")
                if len(transcript) > 3:
                    print("...")
            
            return transcript
            
    except Exception as e:
        print(f"ìë§‰ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def create_full_text(transcript):
    """
    json ë§ê³  í…ìŠ¤íŠ¸ 
    """
    if not transcript:
        return ""
    
    full_text = " ".join([segment['text'] for segment in transcript])
    
    full_text = " ".join(full_text.split())  # Remove extra whitespace
    
    print(f"ì¶”ë¡ ìš© í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(full_text)}ì")
    print(f"í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {full_text[:100]}...")
    
    return full_text

def limit_text_for_2min(text, max_chars=800):
    """
    Limit text to approximately 2 minutes of speech.
    Average speaking rate is about 150-160 words per minute.
    For Korean, approximately 400-500 characters per minute.
    
    Args:
        text: Full text to limit
        max_chars: Maximum characters (approximately 2 minutes)
    
    Returns:
        str: Limited text for 2-minute generation
    """
    if len(text) <= max_chars:
        return text
    
    # Truncate to max_chars and try to end at a sentence boundary
    truncated = text[:max_chars]
    
    # Try to find a good sentence ending
    sentence_endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ']
    last_sentence_end = -1
    
    for ending in sentence_endings:
        pos = truncated.rfind(ending)
        if pos > last_sentence_end:
            last_sentence_end = pos
    
    if last_sentence_end > max_chars * 0.7:  # If we found a sentence end in the last 30%
        limited_text = truncated[:last_sentence_end + 1]
    else:
        # If no good sentence ending, just truncate
        limited_text = truncated
    
    print(f"í…ìŠ¤íŠ¸ 2ë¶„ ì œí•œ ì ìš©: {len(text)}ì -> {len(limited_text)}ì")
    print(f"ì œí•œëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {limited_text[:100]}...")
    
    return limited_text

def create_elevenlabs_voice(audio_file, voice_name, api_key):
    """Create Elevenlabs voice model from audio file"""
    url = "https://api.elevenlabs.io/v1/voices/add"
    
    if not os.path.exists(audio_file):
        print(f"ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {audio_file}")
        return None
        
    print(f"ì˜¤ë””ì˜¤ íŒŒì¼ ì‚¬ìš©: {os.path.abspath(audio_file)}")
    
    headers = {
        "xi-api-key": api_key
    }
    
    files = {
        "files": (os.path.basename(audio_file), open(audio_file, "rb"), "audio/mpeg")
    }
    
    data = {
        "name": voice_name,
        "remove_background_noise": "true"
    }
    
    try:
        print("Elevenlabs ëª¨ë¸ ìƒì„± ì¤‘...")
        response = requests.post(url, headers=headers, files=files, data=data)
        response.raise_for_status()
        
        result = response.json()
        print(f"ìŒì„± ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        print(f"ë³´ì´ìŠ¤ ID: {result['voice_id']}")
        print(f"ê²€ì¦ í•„ìš”: {result['requires_verification']}")
        return result['voice_id']
        
    except requests.exceptions.RequestException as e:
        print(f"ì¼ë ˆë¸ë©ìŠ¤ ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        if hasattr(e.response, 'text'):
            print(f"ì‘ë‹µ: {e.response.text}")
        return None
    finally:
        files["files"][1].close()

def apply_vad(audio_data, sample_rate, aggressiveness=3, frame_duration=30):
    """
    ìŒì„± í™œë™ ê°ì§€(VAD) ì²˜ë¦¬
    """
    print("ìŒì„± í™œë™ ê°ì§€(VAD) ì²˜ë¦¬ ì¤‘...")
    
    # Initialize VAD
    vad = webrtcvad.Vad(aggressiveness)
    
    # Convert to 16-bit PCM
    audio_int16 = (audio_data * 32767).astype(np.int16)
    
    # Calculate frame size
    frame_size = int(sample_rate * frame_duration / 1000)
    
    # Split audio into frames
    frames = []
    for i in range(0, len(audio_int16), frame_size):
        frame = audio_int16[i:i + frame_size]
        if len(frame) == frame_size:  # Only process complete frames
            frames.append(frame)
    
    # Process frames with VAD
    speech_frames = []
    for frame in frames:
        # Convert frame to bytes
        frame_bytes = frame.tobytes()
        try:
            # Check if frame contains speech
            is_speech = vad.is_speech(frame_bytes, sample_rate)
            if is_speech:
                speech_frames.append(frame)
        except Exception as e:
            print(f"VAD ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            continue
    
    if not speech_frames:
        print("ê²½ê³ : VADê°€ ìŒì„±ì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return audio_data
    
    # Merge speech frames
    speech_audio = np.concatenate(speech_frames)
    
    # Convert back to float
    speech_audio = speech_audio.astype(np.float32) / 32767.0
    
    # Calculate duration of removed segments
    original_duration = len(audio_data) / sample_rate
    new_duration = len(speech_audio) / sample_rate
    removed_duration = original_duration - new_duration
    
    print(f"VAD ì²˜ë¦¬ ì™„ë£Œ:")
    print(f"- ì œê±°ëœ ë¬´ìŒ êµ¬ê°„: {removed_duration:.1f}ì´ˆ")
    print(f"- ë‚¨ì€ ìŒì„± êµ¬ê°„: {new_duration:.1f}ì´ˆ")
    
    return speech_audio

def normalize_audio(audio_data, sample_rate, target_rms=-20, target_peak=-3):
    """Normalize audio volume (-23dB to -18dB RMS, -3dB peak)"""
    # Initialize loudness meter
    meter = pyln.Meter(sample_rate)
    # Calculate current loudness
    current_loudness = meter.integrated_loudness(audio_data)
    # Calculate peak level
    current_peak = np.max(np.abs(audio_data))
    current_peak_db = 20 * np.log10(current_peak)
    # Calculate required gain for RMS normalization
    rms_gain = target_rms - current_loudness
    # Calculate required gain for peak normalization
    peak_gain = target_peak - current_peak_db
    # Use smaller gain to prevent clipping
    gain = min(rms_gain, peak_gain)
    # Apply gain
    normalized_audio = audio_data * (10 ** (gain / 20))
    print(f"ìŒëŸ‰ ì •ê·œí™” (RMS: {target_rms}dB, í”¼í¬: {target_peak}dB) ì „ì²˜ë¦¬ ì™„ë£Œ")
    return normalized_audio

def convert_to_mp3(input_file, output_file, bitrate="128k"):
    """Convert WAV to MP3 using ffmpeg"""
    try:
        command = [
            "ffmpeg", "-y",  # -y to overwrite output file if it exists
            "-i", input_file,
            "-codec:a", "libmp3lame",
            "-b:a", bitrate,
            output_file
        ]
        subprocess.run(command, check=True, capture_output=True)
        return True
    except subprocess.CalledProcessError as e:
        print(f"MP3 ë³€í™˜ ì˜¤ë¥˜: {str(e)}")
        return False

def process_audio(input_file, transcript=None, output_file="processed_audio.mp3", target_duration=120):
    """Process audio for Elevenlabs IVC requirements"""
    print("ë©”ì¸ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹œì‘...")
    
    # Verify input file exists
    if not os.path.exists(input_file):
        print(f"ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {input_file}")
        return None
    
    print(f"ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {os.path.abspath(input_file)}")
    
    # Load audio file
    audio_data, sample_rate = sf.read(input_file)
    
    # Convert stereo to mono if needed
    if len(audio_data.shape) > 1:
        print("ìŠ¤í…Œë ˆì˜¤ -> ëª¨ë…¸ë¡œ ë³€í™˜ ì¤‘...")
        audio_data = np.mean(audio_data, axis=1)
    
    # Trim to 2 minutes if longer
    max_samples = target_duration * sample_rate
    if len(audio_data) > max_samples:
        print(f"ì˜¤ë””ì˜¤ ìµœëŒ€ 2ë¶„ìœ¼ë¡œ ìë¦„")
        audio_data = audio_data[:max_samples]
    
    # Apply VAD
    audio_data = apply_vad(audio_data, sample_rate)
    
    # Normalize volume
    normalized_audio = normalize_audio(audio_data, sample_rate)
    
    # Save as temporary WAV file
    temp_wav = "temp_processed.wav"
    sf.write(temp_wav, normalized_audio, sample_rate)
    
    # Convert WAV to MP3
    print("MP3ë¡œ ë³€í™˜ ì¤‘...")
    output_path = os.path.abspath(output_file)
    if not convert_to_mp3(temp_wav, output_path, "128k"):
        return None
    
    # Remove temporary file
    os.remove(temp_wav)
    
    # Check file size
    file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
    if file_size_mb > 11:
        print(f"ê²½ê³ : íŒŒì¼ í¬ê¸°ê°€ 11MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤ ({file_size_mb:.2f}MB)")
        # Try with lower bitrate if file is too large
        print("ë” ë‚®ì€ ë¹„íŠ¸ë ˆì´íŠ¸ë¡œ ë‹¤ì‹œ ì‹œë„ ì¤‘...")
        if not convert_to_mp3(temp_wav, output_path, "96k"):
            return None
        file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
        print(f"ìƒˆ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f}MB")
    
    print(f"ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"íŒŒì¼ í¬ê¸°: {file_size_mb:.2f}MB")
    print(f"ë¹„íŠ¸ë ˆì´íŠ¸: 128kbps")
    
    return output_path

def download_audio(youtube_url, output_file="raw_audio.wav"):
    """Download audio from YouTube video"""
    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': 'temp_audio.%(ext)s',
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'wav',
            'preferredquality': '192',
        }],
        'quiet': True,
        'no_warnings': True,
        'extract_flat': False,
        'force_generic_extractor': False
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            print("ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ ì¤‘...")
            ydl.download([youtube_url])
            
        if os.path.exists("temp_audio.wav"):
            output_path = os.path.abspath(output_file)
            os.rename("temp_audio.wav", output_path)
            print(f"ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path}")
            return output_path
        else:
            print("ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± ì‹¤íŒ¨")
            return None
            
    except yt_dlp.utils.DownloadError as e:
        print(f"ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def calculate_mcd(original_audio_path, generated_audio_path, sample_rate=22050):
    """
    Calculate Mel-Cepstral Distortion (MCD) between original and generated audio.
    
    Args:
        original_audio_path: Path to original audio file
        generated_audio_path: Path to generated audio file
        sample_rate: Sample rate for processing
    
    Returns:
        float: MCD score (lower is better)
    """
    try:
        print(f"MCD ê³„ì‚° ì¤‘...")
        print(f"ì›ë³¸: {original_audio_path}")
        print(f"ìƒì„±: {generated_audio_path}")
        
        # Load audio files
        original_audio, _ = librosa.load(original_audio_path, sr=sample_rate)
        generated_audio, _ = librosa.load(generated_audio_path, sr=sample_rate)
        
        # Ensure both audios have the same length (use the shorter one)
        min_length = min(len(original_audio), len(generated_audio))
        original_audio = original_audio[:min_length]
        generated_audio = generated_audio[:min_length]
        
        # Extract mel-cepstral coefficients
        original_mel = librosa.feature.melspectrogram(y=original_audio, sr=sample_rate)
        generated_mel = librosa.feature.melspectrogram(y=generated_audio, sr=sample_rate)
        
        # Convert to log scale
        original_log_mel = np.log(original_mel + 1e-8)
        generated_log_mel = np.log(generated_mel + 1e-8)
        
        # Calculate MCD
        mcd = np.mean(np.sqrt(2 * np.sum((original_log_mel - generated_log_mel) ** 2, axis=0)))
        
        print(f"MCD ê³„ì‚° ì™„ë£Œ: {mcd:.4f}")
        return mcd
        
    except Exception as e:
        print(f"MCD ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return float('inf')

def evaluate_model_performance(original_audio_path, generated_audio_path, voice_name):
    """
    Evaluate model performance using MCD.
    
    Args:
        original_audio_path: Path to original processed audio
        generated_audio_path: Path to generated audio
        voice_name: Name of the voice model
    
    Returns:
        dict: Performance metrics
    """
    print(f"\n{'='*50}")
    print(f"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€: {voice_name}")
    print(f"{'='*50}")
    
    if not os.path.exists(original_audio_path):
        print(f"âŒ ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {original_audio_path}")
        return None
    
    if not os.path.exists(generated_audio_path):
        print(f"âŒ ìƒì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {generated_audio_path}")
        return None
    
    # Calculate MCD
    mcd_score = calculate_mcd(original_audio_path, generated_audio_path)
    
    # Get file sizes for additional metrics
    original_size = os.path.getsize(original_audio_path) / (1024 * 1024)  # MB
    generated_size = os.path.getsize(generated_audio_path) / (1024 * 1024)  # MB
    
    # Calculate duration
    original_duration = librosa.get_duration(path=original_audio_path)
    generated_duration = librosa.get_duration(path=generated_audio_path)
    
    performance_metrics = {
        'voice_name': voice_name,
        'original_audio_path': original_audio_path,
        'generated_audio_path': generated_audio_path,
        'mcd_score': mcd_score,
        'original_size_mb': original_size,
        'generated_size_mb': generated_size,
        'original_duration': original_duration,
        'generated_duration': generated_duration,
        'duration_difference': abs(original_duration - generated_duration),
        'size_ratio': generated_size / original_size if original_size > 0 else 0
    }
    
    print(f"ì„±ëŠ¥ ì§€í‘œ:")
    print(f"  - MCD ì ìˆ˜: {mcd_score:.4f}")
    print(f"  - ì›ë³¸ í¬ê¸°: {original_size:.2f}MB")
    print(f"  - ìƒì„± í¬ê¸°: {generated_size:.2f}MB")
    print(f"  - ì›ë³¸ ê¸¸ì´: {original_duration:.2f}ì´ˆ")
    print(f"  - ìƒì„± ê¸¸ì´: {generated_duration:.2f}ì´ˆ")
    print(f"  - ê¸¸ì´ ì°¨ì´: {abs(original_duration - generated_duration):.2f}ì´ˆ")
    
    return performance_metrics

def cleanup_generated_files(results, keep_best_only=False):
    """
    Clean up generated audio files after MCD evaluation.
    
    Args:
        results: List of results from voice creation and evaluation
        keep_best_only: If True, keep only the best model's files
    """
    print("ìƒì„±ëœ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    
    files_to_remove = []
    
    for result in results:
        if result['status'] == 'success':
            # Remove generated audio files
            if result['generated_audio_path'] and os.path.exists(result['generated_audio_path']):
                files_to_remove.append(result['generated_audio_path'])
            
            # Remove processed audio files (original training audio)
            processed_audio = f"processed_audio_{result['voice_name']}.mp3"
            if os.path.exists(processed_audio):
                files_to_remove.append(processed_audio)
    
    # If keeping best only, identify the best model
    if keep_best_only and results:
        valid_results = [r for r in results if r['status'] == 'success' and r['performance_metrics']]
        if valid_results:
            best_result = min(valid_results, key=lambda x: x['performance_metrics']['mcd_score'])
            
            # Remove best model's files from cleanup list
            if best_result['generated_audio_path'] in files_to_remove:
                files_to_remove.remove(best_result['generated_audio_path'])
            
            processed_audio = f"processed_audio_{best_result['voice_name']}.mp3"
            if processed_audio in files_to_remove:
                files_to_remove.remove(processed_audio)
            
            print(f"ìµœê³  ëª¨ë¸ íŒŒì¼ ë³´ì¡´: {best_result['voice_name']}")
    
    # Remove files
    for file_path in files_to_remove:
        try:
            os.remove(file_path)
            print(f"ì‚­ì œë¨: {file_path}")
        except Exception as e:
            print(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {str(e)}")
    
    print(f"íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {len(files_to_remove)}ê°œ íŒŒì¼ ì‚­ì œë¨")

def find_best_model(performance_results):
    """
    Find the best model based on MCD scores.
    
    Args:
        performance_results: List of performance metrics dictionaries
    
    Returns:
        dict: Best model information with voice_id
    """
    if not performance_results:
        print("âŒ í‰ê°€í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # Filter out failed evaluations
    valid_results = [r for r in performance_results if r is not None and r['mcd_score'] != float('inf')]
    
    if not valid_results:
        print("âŒ ìœ íš¨í•œ MCD ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # Sort by MCD score (lower is better)
    sorted_results = sorted(valid_results, key=lambda x: x['mcd_score'])
    
    best_model = sorted_results[0]
    
    print(f"\n{'='*60}")
    print("ëª¨ë¸ ì„ ì • ì™„ë£Œ")
    print(f"{'='*60}")
    print(f"ìµœê³  ëª¨ë¸: {best_model['voice_name']}")
    print(f"Voice ID: {best_model['voice_id']}")
    print(f"MCD ì ìˆ˜: {best_model['mcd_score']:.4f}")
    print(f"ì›ë³¸ ì˜¤ë””ì˜¤: {best_model['original_audio_path']}")
    print(f"ìƒì„±ëœ ì˜¤ë””ì˜¤: {best_model['generated_audio_path']}")
    
    print(f"\nì „ì²´ ëª¨ë¸ ìˆœìœ„ (MCD ê¸°ì¤€):")
    for i, result in enumerate(sorted_results, 1):
        print(f"{i}. {result['voice_name']} (ID: {result['voice_id']}): {result['mcd_score']:.4f}")
    
    print(f"{'='*60}")
    
    return best_model

def generate_audio_with_elevenlabs(voice_id, text, api_key, output_file="generated_audio.mp3"):
    """
    Generate audio using Elevenlabs voice model with given text.
    
    Args:
        voice_id: Elevenlabs voice ID
        text: Text to synthesize
        api_key: Elevenlabs API key
        output_file: Output MP3 file name
    
    Returns:
        str: Path to generated audio file if successful, None otherwise
    """
    url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
    
    headers = {
        "xi-api-key": api_key,
        "Content-Type": "application/json"
    }
    
    # Limit text to 2 minutes
    limited_text = limit_text_for_2min(text)
    
    data = {
        "text": limited_text,
        "model_id": "eleven_multilingual_v2",
        "voice_settings": {
            "stability": 0.5,
            "similarity_boost": 0.75
        }
    }
    
    try:
        print(f"Elevenlabs ìŒì„± ì¶”ë¡  ì¤‘... (Voice ID: {voice_id})")
        print(f"í…ìŠ¤íŠ¸: {limited_text[:100]}...")
        
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        
        # Save the audio
        output_path = os.path.abspath(output_file)
        with open(output_path, "wb") as f:
            f.write(response.content)
        
        # Get file size
        file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
        
        print(f"ìŒì„± í•©ì„± ì™„ë£Œ: {output_path}")
        print(f"íŒŒì¼ í¬ê¸°: {file_size_mb:.2f}MB")
        
        return output_path
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Elevenlabs ìŒì„± í•©ì„± ì˜¤ë¥˜: {str(e)}")
        if hasattr(e.response, 'text'):
            print(f"ì‘ë‹µ: {e.response.text}")
        return None
    except Exception as e:
        print(f"âŒ ìŒì„± í•©ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def create_voice_and_evaluate(youtube_url, voice_name, api_key):
    """
    Create voice model, generate audio, and evaluate performance.
    Returns voice_id, generated audio path, and performance metrics.
    """
    print(f"\n{'='*60}")
    print(f"ê°œë³„ Voice ëª¨ë¸ ìƒì„± ë° ì„±ëŠ¥ í‰ê°€: {voice_name}")
    print(f"{'='*60}")
    
    # Extract transcript
    transcript = extract_transcript(youtube_url)
    if not transcript:
        print("âŒ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None, None, None
    
    # Create full text
    full_text = create_full_text(transcript)
    if not full_text:
        print("âŒ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ì–´ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None, None, None
    
    # Download audio
    raw_audio = download_audio(youtube_url)
    if not raw_audio:
        return None, None, None
    
    # Process audio with voice-specific filename
    processed_audio = f"processed_audio_{voice_name}.mp3"
    processed_audio = process_audio(raw_audio, transcript, processed_audio)
    if not processed_audio:
        return None, None, None
    
    # Create voice model
    voice_id = create_elevenlabs_voice(processed_audio, voice_name, api_key)
    if not voice_id:
        return None, None, None
    
    # Generate audio using the model
    generated_audio_path = generate_audio_with_elevenlabs(
        voice_id=voice_id,
        text=full_text,
        api_key=api_key,
        output_file=f"generated_{voice_name}.mp3"
    )
    
    # Evaluate performance
    performance_metrics = None
    if generated_audio_path:
        performance_metrics = evaluate_model_performance(
            processed_audio, generated_audio_path, voice_name
        )
        # Add voice_id to performance metrics
        if performance_metrics:
            performance_metrics['voice_id'] = voice_id
    
    # Cleanup
    if os.path.exists(raw_audio):
        os.remove(raw_audio)
    
    return voice_id, generated_audio_path, performance_metrics

def create_multiple_voices_and_evaluate(youtube_urls, voice_names, api_key):
    """
    Create multiple voices, generate audio, and evaluate performance for each.
    
    Args:
        youtube_urls: list of YouTube URLs
        voice_names: list of voice names
        api_key: Elevenlabs API key
    
    Returns:
        tuple: (results, best_model) where best_model contains voice_id
    """
    if len(youtube_urls) != len(voice_names):
        print("âŒ ì˜¤ë¥˜: YouTube URL ìˆ˜ì™€ Voice Name ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return [], None
    
    results = []
    performance_results = []
    
    for i, (youtube_url, voice_name) in enumerate(zip(youtube_urls, voice_names), 1):
        print(f"\n{'='*60}")
        print(f"ì²˜ë¦¬ ì¤‘: {i}/{len(youtube_urls)}")
        print(f"URL: {youtube_url}")
        print(f"Voice Name: {voice_name}")
        print(f"{'='*60}")
        
        try:
            voice_id, generated_audio_path, performance_metrics = create_voice_and_evaluate(
                youtube_url, voice_name, api_key
            )
            
            if voice_id:
                results.append({
                    'voice_id': voice_id,
                    'voice_name': voice_name,
                    'youtube_url': youtube_url,
                    'generated_audio_path': generated_audio_path,
                    'performance_metrics': performance_metrics,
                    'status': 'success',
                    'index': i
                })
                
                if performance_metrics:
                    performance_results.append(performance_metrics)
                
                print(f"ì„±ê³µ: {voice_name} (ID: {voice_id})")
                if generated_audio_path:
                    print(f"ğŸµ ìƒì„±ëœ ì˜¤ë””ì˜¤: {generated_audio_path}")
                if performance_metrics:
                    print(f"MCD ì ìˆ˜: {performance_metrics['mcd_score']:.4f}")
            else:
                results.append({
                    'voice_id': None,
                    'voice_name': voice_name,
                    'youtube_url': youtube_url,
                    'generated_audio_path': None,
                    'performance_metrics': None,
                    'status': 'failed',
                    'index': i
                })
                print(f"âŒ ì‹¤íŒ¨: {voice_name}")
                
        except Exception as e:
            results.append({
                'voice_id': None,
                'voice_name': voice_name,
                'youtube_url': youtube_url,
                'generated_audio_path': None,
                'performance_metrics': None,
                'status': 'error',
                'error': str(e),
                'index': i
            })
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {voice_name} - {str(e)}")
    
    # Find best model
    best_model = find_best_model(performance_results)
    
    # Clean up generated files (keep only the best model's files)
    cleanup_generated_files(results, keep_best_only=True)
    
    # Print summary
    successful = [r for r in results if r['status'] == 'success']
    failed = [r for r in results if r['status'] != 'success']
    
    print(f"\n{'='*60}")
    print("ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½:")
    print(f"ì´ ì²˜ë¦¬: {len(results)}ê°œ")
    print(f"ì„±ê³µ: {len(successful)}ê°œ")
    print(f"ì‹¤íŒ¨: {len(failed)}ê°œ")
    
    if successful:
        print("\nì„±ê³µí•œ Voice ëª¨ë¸ë“¤:")
        for result in successful:
            print(f"- {result['voice_name']}: {result['voice_id']}")
            if result['performance_metrics']:
                print(f"  MCD: {result['performance_metrics']['mcd_score']:.4f}")
    
    if best_model:
        print(f"\n ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['voice_name']}")
        print(f"   Voice ID: {best_model['voice_id']}")
        print(f"   MCD ì ìˆ˜: {best_model['mcd_score']:.4f}")
    
    print(f"{'='*60}")
    
    return results, best_model